{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "061a92ca-8dcf-48c2-b9a3-414791aa87e6",
      "metadata": {},
      "source": [
        "# Accelerating data operations\n",
        "\n",
        "Dataset - BlocPower\n",
        "\n",
        "This notebook explores methods to speed up and optimize working with large dataframes in Python. Because of compute and storage limitations, we desperately need ways to\n",
        "- make operations faster\n",
        "\n",
        "- consume less memory and/or handle data larger than memory\n",
        "\n",
        "There are three methods tried here:\n",
        "\n",
        "- Pandas: Regular workflow\n",
        "\n",
        "- Dask: Flexible library for parallel computing in Python.\n",
        "\n",
        "- Polars: Pandas alternative using Apache Arrow columnar memory written in Rust\n",
        "\n",
        "The first step is common for all three methods - **define a query in SQL in the python notebook** to refer to our chosen data. This marks a distinct change in our usual approach, where we pre-add a dataset in the project or thru the visual GUI, transform/filter and then import it into the notebook\n",
        "\n",
        "\n",
        "## Results:\n",
        "\n",
        "For the limited use case we loop thru a list of state code (here NY and RI), read in the data, count the number of missing values in each variable, append the dataframes together and perform a simple groupby to calculate the mean Energy Use Intensity in a county.\n",
        "\n",
        "The time and memory taken by each library is given.\n",
        "\n",
        "- **Library |  Time   | Memory**\n",
        "\n",
        "- Pandas  | 36.06s  | 3000 MB\n",
        "\n",
        "- Dask    | 75.33s  | 64 BYTES\n",
        "\n",
        "- Pandas  | 38.66s  | 48 BYTES\n",
        "\n",
        "Documentation says that as datasets get larger, the difference grows and Pandas performs much worse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b7a220",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full documentation available at:\n",
        "# https://apidocs.redivis.com/client-libraries/redivis-python\n",
        "\n",
        "import redivis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import polars as pl\n",
        "import sys as sys\n",
        "import dask.dataframe as dd\n",
        "import dask.bag as db\n",
        "from dask.delayed import delayed\n",
        "import time as time\n",
        "import dask.array as da\n",
        "\n",
        "\n",
        "!conda install -y polars\n",
        "import polars as pl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810b54ba-bdae-4489-aeee-ccbb0959cbfb",
      "metadata": {},
      "source": [
        "# DASK\n",
        "## delayed\n",
        "Dask Delayed is a way to create Dask graphs for parallel computing. It allows you to parallelize computations on a single machine by creating a task graph of the operations that need to be performed, and then executing the graph in parallel.\n",
        "\n",
        "When you use Dask Delayed, you're essentially creating a lazy evaluation graph, where each computation is represented by a delayed object. The delayed object is a wrapper around a Python function call that hasn't been executed yet. The function call is only executed when you trigger the computation using the compute() method.\n",
        "\n",
        "Dask Delayed works by breaking down your computations into a series of tasks that can be executed independently. Each task is represented by a delayed object, which is added to the task graph. Once the entire task graph has been constructed, you can execute it in parallel by calling dask.compute() on the delayed objects.\n",
        "\n",
        "Dask Delayed is useful when you have a large computation that can be broken down into smaller, independent computations. By using Dask Delayed, you can avoid loading the entire dataset into memory and instead process it in smaller chunks, which can be more efficient. It also allows you to parallelize your computations across multiple cores or machines, which can dramatically speed up your analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf6af9a-9ceb-41d5-827f-f11f40409f38",
      "metadata": {},
      "source": [
        "\n",
        "the delayed() function is used to create a delayed object for each query result. The delayed objects are collected in a list dfs. Finally, the from_delayed() function is used to create the dask dataframe dask_df. Note that you can use dask's distributed computing capabilities to read the data more efficiently in parallel, by setting up a dask cluster and submitting the delayed objects to it using the dask.distributed module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c8d256e-f78a-4baa-8beb-17f52adee44e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "st1= time.time()\n",
        "states = ['NY', 'RI']\n",
        "dfs = []\n",
        "for state in states:\n",
        "    query = redivis.query(f\"\"\"\n",
        "        SELECT * \n",
        "        FROM EIDC.blocpower_active.blocpower_core\n",
        "        WHERE state = '{state}'\n",
        "        \"\"\")\n",
        "#convert query to dask delayed    \n",
        "    delayed_df = delayed(query.to_dataframe)()\n",
        "    print(f\"Memory consumed by DASK Delayed DF for {state}: {sys.getsizeof(delayed_df)} bytes\")\n",
        "    missing_values_count = delayed_df.isna().sum().compute()\n",
        "    print(\"######################################\")\n",
        "    print(f\"Missingness for {state}\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(missing_values_count)\n",
        "    print(\"######################################\")\n",
        "\n",
        "    dfs.append(delayed_df)\n",
        "    \n",
        "dfs = dd.from_delayed(dfs)\n",
        "    \n",
        "dfcounty = dfs.groupby(\"county\").total_source_energy_GJ.mean().compute()\n",
        "      \n",
        "et1=time.time()\n",
        "\n",
        "dur1 = et1-st1\n",
        "print(\"time Dask -\", dur1)\n",
        "\n",
        "dask_df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b711dda3-c8e5-43c5-8b16-cbd23e545e67",
      "metadata": {},
      "source": [
        "# Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1621bb6b-bdae-4ba5-9ccd-7d3f754beadd",
      "metadata": {},
      "outputs": [],
      "source": [
        "st2=time.time()\n",
        "states = ['DC']\n",
        "dfs = []\n",
        "for state in states:\n",
        "    query = redivis.query(f\"\"\"\n",
        "        SELECT * \n",
        "        FROM EIDC.blocpower_active.blocpower_core\n",
        "        WHERE state = '{state}'\n",
        "        \"\"\")\n",
        "    df = query.to_dataframe()\n",
        "    size_in_mb = sys.getsizeof(df)/ (1024**2)\n",
        "    print(f\"Memory consumed by Pandas DF for {state}: {size_in_mb:.2f} MB\")\n",
        "    \n",
        "    missing_values_count = df.isna().sum()\n",
        "    \n",
        "    print(\"######################################\")\n",
        "    print(f\"Missingness for {state}\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(missing_values_count)\n",
        "    dfs.append(df)\n",
        "    \n",
        "pandas_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "dfcounty = pandas_df.groupby('county').total_source_energy_GJ.mean()\n",
        "\n",
        "et2=time.time()\n",
        "\n",
        "dur2 = et2-st2\n",
        "\n",
        "print(\"time pandas -\", dur2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb3a3be-60dd-4fc5-8288-6242db00481d",
      "metadata": {},
      "outputs": [],
      "source": [
        "st2=time.time()\n",
        "states = ['DC']\n",
        "dfs = []\n",
        "for state in states:\n",
        "    query = redivis.query(f\"\"\"\n",
        "        SELECT * \n",
        "        FROM EIDC.blocpower_active.blocpower_core\n",
        "        WHERE state = '{state}'\n",
        "        \"\"\")\n",
        "    df = query.to_dataframe()\n",
        "    size_in_mb = sys.getsizeof(df)/ (1024**2)\n",
        "    print(f\"Memory consumed by Pandas DF for {state}: {size_in_mb:.2f} MB\")\n",
        "    \n",
        "    missing_values_count = df.isna().sum()\n",
        "    \n",
        "    uniques = df.nunique()\n",
        "\n",
        "    print(\"######################################\")\n",
        "    print(f\"Missingness for {state}\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(missing_values_count)\n",
        "    print(\"-------------------------------------\")\n",
        "    print(uniques)\n",
        "    dfs.append(df)\n",
        "    \n",
        "pandas_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "\n",
        "et2=time.time()\n",
        "\n",
        "dur2 = et2-st2\n",
        "\n",
        "print(\"time pandas -\", dur2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733654a9-8d74-44c4-8934-d1e927e3a9c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea109870-5d4e-4f9f-a768-cf1c434496ad",
      "metadata": {},
      "source": [
        "# Polars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682158d0-f868-4639-b328-3263b1efcc2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://towardsdatascience.com/understanding-groupby-in-polars-dataframe-by-examples-1e910e4095b3\n",
        "\n",
        "st2=time.time()\n",
        "\n",
        "states = ['NY', 'RI']\n",
        "dfs = []\n",
        "for state in states:\n",
        "    query = redivis.query(f\"\"\"\n",
        "        SELECT * \n",
        "        FROM EIDC.blocpower_active.blocpower_core\n",
        "        WHERE state = '{state}'\n",
        "        \"\"\")\n",
        "    #CONVERT TO PANDAS\n",
        "    df = query.to_dataframe()\n",
        "    \n",
        "    #CONVERT TO POLARS\n",
        "    df = pl.from_pandas(df)\n",
        "    #GET SIZE IN MEMORY\n",
        "    print(f\"Memory consumed by Polars DF for {state}: {sys.getsizeof(delayed_df):.2f} BYTES \")\n",
        "    \n",
        "    null_count_df=df.null_count().to_pandas()\n",
        "    \n",
        "    print(null_count_df)\n",
        "    dfs.append(df)\n",
        "    \n",
        "polars_df = pl.concat(dfs)\n",
        "\n",
        "q = (\n",
        "    polars_df    \n",
        "    .lazy()\n",
        "    .groupby(by='county')\n",
        "    .agg(\n",
        "        [\n",
        "            pl.col('total_source_energy_GJ').mean().alias('mean_energy'),\n",
        "            \n",
        "        ]\n",
        "    )    \n",
        ")\n",
        "\n",
        "polars_df = q.collect()\n",
        "\n",
        "# q = (\n",
        "#     polars_df.lazy()\n",
        "#     .groupby(\"county\")\n",
        "#     .agg(mean_energy=('total_source_energy_GJ', pl.mean()))\n",
        "# )\n",
        "\n",
        "et2=time.time()\n",
        "\n",
        "dur2 = et2-st2\n",
        "print(\"time polars -\", dur2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9bc55b2-8cb5-4f06-b89b-51271246a17d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "states = [\"DC\", \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n",
        "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
        "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
        "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
        "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
        "dfs = []\n",
        "for state in states:\n",
        "    query = redivis.query(f\"\"\"\n",
        "        SELECT * \n",
        "        FROM EIDC.blocpower_active.blocpower_core\n",
        "        WHERE state = '{state}'\n",
        "        \"\"\")\n",
        "    #CONVERT TO PANDAS\n",
        "    df = query.to_dataframe()\n",
        "    \n",
        "    #CONVERT TO POLARS\n",
        "    df = pl.from_pandas(df)\n",
        "    #GET SIZE IN MEMORY\n",
        "    print(f\"Memory consumed by Polars DF for {state}: {sys.getsizeof(delayed_df):.2f} BYTES \")\n",
        "    \n",
        "    null_count_df=df.null_count().to_pandas()\n",
        "    \n",
        "    print(null_count_df)\n",
        "    dfs.append(df)\n",
        "    \n",
        "polars_df = pl.concat(dfs)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}